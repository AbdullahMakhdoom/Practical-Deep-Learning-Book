{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud APIs for Computer Vision: Up and Running in 15 Minutes\n",
    "\n",
    "This code is part of [Chapter 8- Cloud APIs for Computer Vision: Up and Running in 15 Minutes ](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch08.html).\n",
    "\n",
    "## Compile Results for Image Tagging\n",
    "\n",
    "In this file we will compile the results using the ground truth and the collected data for all the test images. You will need to edit the following: \n",
    "\n",
    "1. Please edit `PATH_TO_IMAGES` with the path to the test images that have been used for the experiments. \n",
    "2. If you used different filenames for the prediction filenames, please edit the filenames accordingly.\n",
    "3. Please download Gensim, which we will be using for comparing word similarity between ground truth with predicted class. The paths to the `GoogleNews-vectors-negative300.bin` will need to be updated in the following cells. You can even try using ConceptNet, a feature [currently unavailble in Gensim](https://github.com/RaRe-Technologies/gensim/issues/1296)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ground truth JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/final_gt_tags.json\") as json_file:\n",
    "    ground_truth = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to get image name from image id and converse.\n",
    "def get_id_from_name(name):\n",
    "    return int(name.split(\"/\")[-1].split(\".jpg\")[0])\n",
    "\n",
    "\n",
    "def get_name_from_id(imgId):\n",
    "    filename = \"PATH_TO_IMAGES\" + \\\n",
    "        \"000000\" + str(imgId) + \".jpg\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class ids to their string equivalent\n",
    "with open('./data/clsid_name.json') as f:\n",
    "    clsid_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names to their class ids\n",
    "with open('./data/name_clsid.json') as f:\n",
    "    name_clsid = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'57': 'carrot', '7': 'train', '24': 'zebra', '53': 'apple', '20': 'sheep', '59': 'pizza', '90': 'toothbrush', '56': 'broccoli', '48': 'fork', '5': 'airplane', '61': 'cake', '34': 'frisbee', '70': 'toilet', '44': 'bottle', '8': 'truck', '84': 'book', '13': 'stop sign', '22': 'elephant', '79': 'oven', '55': 'orange', '88': 'teddy bear', '64': 'potted plant', '77': 'cell phone', '6': 'bus', '85': 'clock', '78': 'microwave', '32': 'tie', '31': 'handbag', '72': 'tv', '35': 'skis', '3': 'car', '50': 'spoon', '62': 'chair', '65': 'bed', '16': 'bird', '75': 'remote', '15': 'bench', '4': 'motorcycle', '67': 'dining table', '80': 'toaster', '10': 'traffic light', '18': 'dog', '21': 'cow', '36': 'snowboard', '81': 'sink', '51': 'bowl', '76': 'keyboard', '47': 'cup', '89': 'hair drier', '14': 'parking meter', '49': 'knife', '39': 'baseball bat', '17': 'cat', '19': 'horse', '40': 'baseball glove', '86': 'vase', '37': 'sports ball', '58': 'hot dog', '1': 'person', '73': 'laptop', '74': 'mouse', '23': 'bear', '11': 'fire hydrant', '25': 'giraffe', '41': 'skateboard', '52': 'banana', '63': 'couch', '54': 'sandwich', '28': 'umbrella', '9': 'boat', '38': 'kite', '82': 'refrigerator', '33': 'suitcase', '46': 'wine glass', '2': 'bicycle', '43': 'tennis racket', '60': 'donut', '87': 'scissors', '42': 'surfboard', '27': 'backpack'}\n"
     ]
    }
   ],
   "source": [
    "print(clsid_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_clsid_to_str(l):\n",
    "    # l is of the format [clsid, clsid ..]\n",
    "    result = []\n",
    "    for clsid in l:\n",
    "        result.append(clsid_name[str(clsid)])\n",
    "    #assert len(l) == len(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(l):\n",
    "    l1 = []\n",
    "    for each in l:\n",
    "        if len(each) >= 2:\n",
    "            l1.append(each.lower())\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_from_pred(l):\n",
    "    # l = [[cls, 33], [cls, 88], ..]\n",
    "    return list([item[0] for item in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download Gensim, which we will be using for comparing word similarity between ground truth with predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'PATH TO GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gensim(word, pred):\n",
    "    # get similarity between word and all predicted words in returned predictions\n",
    "    similarity = 0\n",
    "    for each_pred in pred:\n",
    "        # check if returned prediction exists in the Word2Vec model\n",
    "        if each_pred not in model:\n",
    "            continue\n",
    "        current_similarity = model.similarity(word, each_pred)\n",
    "        #print(\"Word=\\t\", word, \"\\tPred=\\t\", each_pred, \"\\tSim=\\t\", current_similarity)\n",
    "        if current_similarity > similarity:\n",
    "            similarity = current_similarity\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Parsing\n",
    "\n",
    "Each cloud provider sends the results in slightly different formats and we need to parse each of them correctly. So, we will develop a parsing function unique to each cloud provider.\n",
    "\n",
    "#### Microsoft Specific Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mystring.replace(\" \", \"_\")\n",
    "\n",
    "def msft_name(imgId):\n",
    "    return \"000000\" + str(imgId) + \".jpg\"\n",
    "\n",
    "def parse_msft_inner(word):\n",
    "    b = word.replace(\"_\", \" \")\n",
    "    c = b.lower().strip().split()\n",
    "    return c\n",
    "\n",
    "def parse_msft(l):\n",
    "    result = []\n",
    "    b = \"\"\n",
    "    for each in l[\"categories\"]:\n",
    "        a = each[\"name\"]\n",
    "        result.extend(parse_msft_inner(a))\n",
    "    for each in l[\"tags\"]:\n",
    "        a = each[\"name\"]\n",
    "        result.extend(parse_msft_inner(a))\n",
    "        if \"hint\" in each:\n",
    "            a = each[\"hint\"]\n",
    "            result.extend(parse_msft_inner(a))\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon Specific Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ama(l):\n",
    "    result = []\n",
    "    for each in l:\n",
    "        result.append(each.lower())\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google specific parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_goog(l):\n",
    "    l1 = []\n",
    "    for each in l:\n",
    "        l1.append(each[0].lower())\n",
    "        if len(each[0].split()) > 1:\n",
    "            l1.extend(each[0].split())\n",
    "    return l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `threshold` defines how much similar do two words (ground truth and predicted category name) need to be according to Word2Vec for the prediction to be a correct prediction. You can play around with the `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = .3\n",
    "\n",
    "# variables to compute average number of predictions\n",
    "avg_gt_len = 0\n",
    "avg_ama_len = 0\n",
    "avg_msft_len = 0\n",
    "avg_goog_len = 0\n",
    "\n",
    "\n",
    "def calculate_score(ground_truth, predictions, arg):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    avg_gt_len = 0\n",
    "    avg_ama_len = 0\n",
    "    avg_msft_len = 0\n",
    "    avg_goog_len = 0\n",
    "    for each in ground_truth.keys():\n",
    "        gt = list(set(convert_clsid_to_str(ground_truth[each])))\n",
    "        if gt == None or len(gt) < 1:\n",
    "            continue\n",
    "        total += len(gt)\n",
    "        avg_gt_len += len(gt)\n",
    "        if arg == \"goog\":\n",
    "            pred = predictions[get_name_from_id(each)]\n",
    "            if pred == None or len(pred) <= 0:\n",
    "                continue\n",
    "            pred = parse_goog(predictions[get_name_from_id(each)])\n",
    "            avg_goog_len += len(pred)\n",
    "        elif arg == \"msft\":\n",
    "            pred = predictions[msft_name(each)]\n",
    "            if pred == None or len(pred) <= 0:\n",
    "                continue\n",
    "            pred = parse_msft(predictions[msft_name(each)])\n",
    "            avg_msft_len += len(pred)\n",
    "        elif arg == \"ama\":\n",
    "            pred = predictions[get_name_from_id(each)]\n",
    "            if pred == None or len(pred) <= 0:\n",
    "                continue\n",
    "            pred = parse_ama(predictions[get_name_from_id(each)])\n",
    "            avg_ama_len += len(pred)\n",
    "        match = 0\n",
    "        match_word = []\n",
    "        for each_word in gt:\n",
    "            # Check if ground truth exists \"as is\" in the entire list of predictions\n",
    "            if each_word in pred:\n",
    "                correct += 1\n",
    "                match += 1\n",
    "                match_word.append(each_word)\n",
    "            # Also, ensure that ground truth exists in the Word2Vec model\n",
    "            elif each_word not in model:\n",
    "                continue\n",
    "            # Otherwise, check for similarity between the ground truth and the predictions\n",
    "            elif check_gensim(each_word, pred) >= threshold:\n",
    "                correct += 1\n",
    "                match += 1\n",
    "                match_word.append(each_word)\n",
    "    if arg == \"goog\":\n",
    "        print(\"Google's Stats\\nTotal number of tags returned = \", avg_goog_len,\n",
    "              \"\\nAverage number of tags returned per image = \",\n",
    "              avg_goog_len * 1.0 / len(ground_truth.keys()))\n",
    "    elif arg == \"ama\":\n",
    "        print(\"Amazon's Stats\\nTotal number of tags returned = \", avg_ama_len,\n",
    "              \"\\nAverage number of tags returned per image = \",\n",
    "              avg_ama_len * 1.0 / len(ground_truth.keys()))\n",
    "    elif arg == \"msft\":\n",
    "        print(\"Microsoft's Stats\\nTotal number of tags returned = \",\n",
    "              avg_msft_len, \"\\nAverage number of tags returned per image = \",\n",
    "              avg_msft_len * 1.0 / len(ground_truth.keys()))\n",
    "    print(\"\\nGround Truth Stats\\nTotal number of Ground Truth tags = \", total,\n",
    "          \"\\nTotal number of correct tags predicted = \", correct)\n",
    "    print(\"\\nScore = \", float(correct) / float(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to load the predictions that we obtained by using APIs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google\n",
    "with open('./data/google_tags.json') as f:\n",
    "    google = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google's Stats\n",
      "Total number of tags returned =  59959 \n",
      "Average number of tags returned per image =  14.602776424744278\n",
      "\n",
      "Ground Truth Stats\n",
      "Total number of Ground Truth tags =  12081 \n",
      "Total number of correct tags predicted =  5754\n",
      "\n",
      "Score =  0.47628507573876333\n"
     ]
    }
   ],
   "source": [
    "# Get Google Score\n",
    "calculate_score(ground_truth, google, \"goog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Microsoft's API for object classification has two versions. The results from both the APIs are different. \n",
    "\n",
    "If you want to check out Microsoft's outdated (v1) API then use the `microsoft_tags.json` file. We will be using the latest version (i.e., `microsoft_tags_DESCRIPTION.json`) for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microsoft\n",
    "with open('../data_files/microsoft_tags_DESCRIPTION.json') as f:\n",
    "    microsoft = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft's Stats\n",
      "Total number of tags returned =  34398 \n",
      "Average number of tags returned per image =  8.377496346809547\n",
      "\n",
      "Ground Truth Stats\n",
      "Total number of Ground Truth tags =  12081 \n",
      "Total number of correct tags predicted =  6033\n",
      "\n",
      "Score =  0.4993791904643655\n"
     ]
    }
   ],
   "source": [
    "# Get Microsoft score\n",
    "calculate_score(ground_truth, microsoft, \"msft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon\n",
    "with open('../data_files/amazon_tags.json') as f:\n",
    "    amazon = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon's Stats\n",
      "Total number of tags returned =  58512 \n",
      "Average number of tags returned per image =  14.250365319045299\n",
      "\n",
      "Ground Truth Stats\n",
      "Total number of Ground Truth tags =  12081 \n",
      "Total number of correct tags predicted =  7859\n",
      "\n",
      "Score =  0.6505256187401706\n"
     ]
    }
   ],
   "source": [
    "# Get Amazon score\n",
    "calculate_score(ground_truth, amazon, \"ama\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
